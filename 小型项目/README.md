学习内容：

完成爬取斗图啦表情包，涉及
1.爬虫基础知识

所用到的第三库有requests,bs4(BeautifulSoup),os,threading

都是做爬虫基本的第三方库

爬虫最主要由几个模块组成：

第一部分：请求数据

第二部分：获取数据（获取链接并进行解析）

第三部分：保存数据到本地

一般简单的爬虫就由这几部分组成

更高级的爬虫程序，则必须考虑网站的防守（反爬）措施，制定相应的进攻（反反爬）策略。

2.多线程

简单的爬取程序，或者说爬取量比较小的，可能并不明显。一旦爬取大量数据，直接爬取下载速度十分的慢，这时就需要多线程来解决这一问题。

顺便学学多线程爬虫编程

3.生产者和消费者模式（下的多线程）

生产者模式和消费者模式会让程序更加整洁美观，即一个爬虫程序都必须是有生产和消费对象的。

下面直接详细说一下这个项目，感觉这个项目还是算通俗易懂的。

回顾一下全程，全程大致步骤包括：

请求数据：使用requests库对网站发出请求，当然不能明目张胆，必须做简单的伪装（目前只会简单的伪装）

headers使用网页中源码里面的User-Agent：……requests先用get()方法去获得每一页的网页源码，存储在一个列表中。然后使用BeautifulSoup对网页进行解析，并把结果返回，这时就已经初步获得了解析后的网页源码，但面对一堆链接和其他东西，我们必须取出我们想要的东西。

获取数据：

所以，我使用bs4的方法去找到符合我们条件的数据信息，过滤掉无用的信息。即soup.find_all()     #这里的soup就是之前解析后的返回值。

此时，信息筛选完便要去找到我们想要的图片真正的超链接。这是核心步骤之一，一般情况都要对解析后的数据进行数据分析，找出爬取规律和思路，认准我们需要的数据的超链接或者地址（否则爬到的都会是无效数据）。而且class类如果后面有loaded记得去掉，这是动态读取时用到的，这里我们必须去掉。找到真正的img地址后，接下来便是准备下载了。

保存本地：

运用os库把图片都下载到指定位置处，使用request.urlretrieve(),再加个提示即可看出图片下载进度。(注意不是requests，这里用到的是os库里面的request方法，而不是requests库)

另外，关于多线程，此处使用了５个/10个线程同时下载（通过遍历即可实现），相比排队下载速度简直快多了。多线程使用的库不需要下载，直接

import threading就ok；还有，关于对全局变量的修改应该注意的问题，在多线程里面，修改全局变量需要及时上锁，及时解锁，防止读取到脏数据。

上锁使用gLock = threading.Lock()

gLock.acquire()上锁

gLock.release()解锁

记得及时解锁，不能妨碍其他变量等的读取。

对于多线程还要继续学习，以及后面的scrapy库的学习（也有多线程的应用）

至于生产者模式与消费者模式，就是对代码重构，使得代码更加美观和简洁。
－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－

与上一篇相比，本项目相对比爬表情包相对难一点。上一个爬取项目是直接解析网页源码即可找到表情包的超链接，而本项目需要通过两层解析，才能找到分辨率为（1920*1080）的高清壁纸。为何如此？因为第一层解析得到的data_original（也就是图片地址）其实不是高清图，都是低画质图片还有一些.png的标题图，而真正想要获得高清壁纸的话，要进行再次解析。也就是把第一层解析出来的数据做数据分析，找出提取规律，提取出高清壁纸所在的网址。最后才能找到图片的超链接进行下载。

第一层解析网页源码和下载保存数据到本地这两个步骤与上篇爬取表情包的步骤方法是一样的，原理可直接参照：

https://blog.csdn.net/honorwh/article/details/88659738
